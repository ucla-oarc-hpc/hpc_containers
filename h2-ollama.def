Bootstrap: docker
From: ubuntu:22.04

%post
	set -ex
	export DEBIAN_FRONTEND=noninteractive
	apt-get update -yq
	apt-get install -yq curl iproute2 bc wget gcc g++ git  make cmake libcurl4-openssl-dev zlib1g-dev m4 libssl-dev libffi-dev libsqlite3-dev libbz2-dev liblzma-dev
	curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o - | tar -C /usr -xzf - 

	wget https://www.python.org/ftp/python/3.11.11/Python-3.11.11.tgz
	tar -vxf Python-3.11.11.tgz ; rm Python-3.11.11.tgz
	cd Python-3.11.11 ; ./configure --enable-shared 
	make ; make install
	export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
	pip3 install --no-cache-dir  open-webui

	apt-get clean -yq
	rm -rf /var/lib/apt/lists/*

%test
	ollama --help


%environment
export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH

%runscript 
#!/bin/bash

set -e

ollama_port="${ollama_port:-11434}"

# Check for an available port
while ss -tulpn 2>/dev/null | grep -q ":${ollama_port}"; do
    ollama_port=$((ollama_port + 1))
done
export OLLAMA_PORT=$ollama_port
export HOST=$(hostname)

# Set up OLLAMA_HOST
export OLLAMA_HOST="127.0.0.1:${OLLAMA_PORT}"

# Set up OLLAMA_MODELS directory
if [ -z "${OLLAMA_MODELS}" ]; then
    export OLLAMA_MODELS="$HOME/ollama_models"
    mkdir -pv "$OLLAMA_MODELS"
fi

# Output information
echo -e "OLLAMA Running on ${HOST} with port ${OLLAMA_PORT}"
echo "OLLAMA_MODELS is $OLLAMA_MODELS"
export OLLAMA_BASE_URL="http://localhost:${OLLAMA_PORT}"
echo "${OLLAMA_BASE_URL}"

if [ $# -eq 0 ] || [ "$1" == "openwebui" ]; then
    echo "Starting ollama"
    ollama serve &

    if [ "$1" == "openwebui" ]; then
	if [ -z "${DATA_DIR}" ]; then
	    export DATA_DIR="$HOME/webui/data"
	    mkdir -pv "$DATA_DIR"
	fi

        # Check for an available port
        webui_port="${webui_port:-8081}"
        while ss -tulpn 2>/dev/null | grep -q ":${webui_port}"; do
            webui_port=$((webui_port + 1))
        done
        echo "Running Open WebUI with port ${webui_port}"
        open-webui serve --port ${webui_port} &
    fi
else
    exec "$@"
fi

%help
	This is a simple container for running Ollama. For more information, visit https://ollama.ai.

